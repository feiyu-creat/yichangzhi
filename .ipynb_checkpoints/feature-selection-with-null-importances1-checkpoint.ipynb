{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f0688e506bb25201d67f33b3ff9c9a2f5bd5e30"
   },
   "source": [
    "### Feature selecture using target permutation\n",
    "\n",
    "The notebook uses a procedure described in [this article]( https://academic.oup.com/bioinformatics/article/26/10/1340/193348).\n",
    "\n",
    "Feature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target).\n",
    "\n",
    "The notebook implements the following steps  :\n",
    " - Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n",
    " - Fit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null Importances Distribution\n",
    " - for each feature test the actual importance:\n",
    "    - Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n",
    "    - Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.\n",
    "\n",
    "For processing time reasons, the notebook will only cover application_train.csv but you can extend it as you wish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7e08c832a50bf4fbd4476d1cebd943ee230be3f"
   },
   "source": [
    "### Import a few packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7421e2121aa6c9229bd4cfeb961912cfefc88e5d"
   },
   "source": [
    "### Read application_train\n",
    "\n",
    "Read data and take care of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('application_train.csv')\n",
    "\n",
    "categorical_feats = [\n",
    "    f for f in data.columns if data[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "categorical_feats\n",
    "for f_ in categorical_feats:#将字符串特征的类别转换为ctaegory(数字)特征类别\n",
    "    data[f_], _ = pd.factorize(data[f_])\n",
    "    # Set feature type as categorical\n",
    "    data[f_] = data[f_].astype('category')\n",
    "#     print(data[f_])\n",
    "# categorical_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d800326c1a3fcb3e8be2d0051cbde2c1927d9ab3"
   },
   "source": [
    "### Create a scoring function\n",
    "\n",
    "Coring function uses LightGBM in RandomForest mode fitted on the full dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d78a5c60c870b15590476088da546be03e2fa6ee"
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(data, shuffle, seed=None):#训练lgb模型，创建特征重要性输出到imp_df\n",
    "    # Gather real features\n",
    "    train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']]\n",
    "    # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "    # Shuffle target if required\n",
    "    y = data['TARGET'].copy()\n",
    "    if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "        y = data['TARGET'].copy().sample(frac=1.0)#指定随机抽取行的比例\n",
    "    \n",
    "    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': seed,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': 4,\n",
    "        'verbose':-1\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = list(train_features)\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features]))\n",
    "    \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e6f4d74f63ec0961d9b76b8b3b60b4fb7b47511"
   },
   "source": [
    "### Build the benchmark for feature importance\n",
    "\n",
    "![](http://)The original paper does not talk about this but I think it makes sense to have a distribution of actual importances as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "38c932d1b6422bcedcc0ab378f8cc4e062393478"
   },
   "outputs": [],
   "source": [
    "# Seed the unexpected randomness of this world\n",
    "np.random.seed(123)\n",
    "# Get the actual importance, i.e. without shuffling\n",
    "actual_imp_df = get_feature_importances(data=data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "3fce92f2e7c467aec384d21584eb271e88fc4d4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_gain</th>\n",
       "      <th>importance_split</th>\n",
       "      <th>trn_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>4855.592482</td>\n",
       "      <td>137</td>\n",
       "      <td>0.757312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>24112.439761</td>\n",
       "      <td>534</td>\n",
       "      <td>0.757312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance_gain  importance_split  trn_score\n",
       "0  NAME_CONTRACT_TYPE      4855.592482               137   0.757312\n",
       "1         CODE_GENDER     24112.439761               534   0.757312"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_imp_df.head(n=2)#返回对象的前n行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(actual_imp_df.shape[0])#特征数120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93791d0a1bd477a19a1fbf7c61e0f2f621bc78bb"
   },
   "source": [
    "### Build Null Importances distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2332391f11aa881124c49e072c71eb58297c6813",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null_imp_df000 Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Done with    1 of    3 (Spent   0.3 min)120\n",
      "null_imp_df000                         feature  importance_gain  importance_split  trn_score  \\\n",
      "0            NAME_CONTRACT_TYPE       219.187219                31   0.701949   \n",
      "1                   CODE_GENDER       399.702142                81   0.701949   \n",
      "2                  FLAG_OWN_CAR       131.112188                36   0.701949   \n",
      "3               FLAG_OWN_REALTY       409.175080                57   0.701949   \n",
      "4                  CNT_CHILDREN       612.259304                67   0.701949   \n",
      "..                          ...              ...               ...        ...   \n",
      "115   AMT_REQ_CREDIT_BUREAU_DAY       370.547534                34   0.701949   \n",
      "116  AMT_REQ_CREDIT_BUREAU_WEEK       491.937580                45   0.701949   \n",
      "117   AMT_REQ_CREDIT_BUREAU_MON      1396.375953               135   0.701949   \n",
      "118   AMT_REQ_CREDIT_BUREAU_QRT       915.401231                89   0.701949   \n",
      "119  AMT_REQ_CREDIT_BUREAU_YEAR      2869.680361               261   0.701949   \n",
      "\n",
      "     run  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "..   ...  \n",
      "115    1  \n",
      "116    1  \n",
      "117    1  \n",
      "118    1  \n",
      "119    1  \n",
      "\n",
      "[120 rows x 5 columns]\n",
      "Done with    2 of    3 (Spent   0.6 min)240\n",
      "null_imp_df000                         feature  importance_gain  importance_split  trn_score  \\\n",
      "0            NAME_CONTRACT_TYPE       219.187219                31   0.701949   \n",
      "1                   CODE_GENDER       399.702142                81   0.701949   \n",
      "2                  FLAG_OWN_CAR       131.112188                36   0.701949   \n",
      "3               FLAG_OWN_REALTY       409.175080                57   0.701949   \n",
      "4                  CNT_CHILDREN       612.259304                67   0.701949   \n",
      "..                          ...              ...               ...        ...   \n",
      "115   AMT_REQ_CREDIT_BUREAU_DAY        91.446230                 9   0.682864   \n",
      "116  AMT_REQ_CREDIT_BUREAU_WEEK       208.689639                24   0.682864   \n",
      "117   AMT_REQ_CREDIT_BUREAU_MON      1226.808797               113   0.682864   \n",
      "118   AMT_REQ_CREDIT_BUREAU_QRT       944.333080                96   0.682864   \n",
      "119  AMT_REQ_CREDIT_BUREAU_YEAR      3426.871513               335   0.682864   \n",
      "\n",
      "     run  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "..   ...  \n",
      "115    2  \n",
      "116    2  \n",
      "117    2  \n",
      "118    2  \n",
      "119    2  \n",
      "\n",
      "[240 rows x 5 columns]\n",
      "Done with    3 of    3 (Spent   0.8 min)360\n"
     ]
    }
   ],
   "source": [
    "null_imp_df = pd.DataFrame()#存贮null_importances特征\n",
    "nb_runs = 3\n",
    "import time\n",
    "start = time.time()\n",
    "dsp = ''\n",
    "for i in range(nb_runs):\n",
    "    # Get current run importances\n",
    "    imp_df = get_feature_importances(data=data, shuffle=True)\n",
    "    imp_df['run'] = i + 1 \n",
    "    print('null_imp_df000',null_imp_df)\n",
    "    # Concat the latest importances with the old ones\n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)#纵向拼接,将每一轮计算到的imp_df中的数据拼到到null_imp_df末尾\n",
    "    # Erase previous message\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)\n",
    "    print(null_imp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "96d51a41b38921ac84ba0853b935a3645afe3939"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_gain</th>\n",
       "      <th>importance_split</th>\n",
       "      <th>trn_score</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>219.187219</td>\n",
       "      <td>31</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>399.702142</td>\n",
       "      <td>81</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLAG_OWN_CAR</td>\n",
       "      <td>131.112188</td>\n",
       "      <td>36</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLAG_OWN_REALTY</td>\n",
       "      <td>409.175080</td>\n",
       "      <td>57</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNT_CHILDREN</td>\n",
       "      <td>612.259304</td>\n",
       "      <td>67</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance_gain  importance_split  trn_score  run\n",
       "0  NAME_CONTRACT_TYPE       219.187219                31   0.701949    1\n",
       "1         CODE_GENDER       399.702142                81   0.701949    1\n",
       "2        FLAG_OWN_CAR       131.112188                36   0.701949    1\n",
       "3     FLAG_OWN_REALTY       409.175080                57   0.701949    1\n",
       "4        CNT_CHILDREN       612.259304                67   0.701949    1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_gain</th>\n",
       "      <th>importance_split</th>\n",
       "      <th>trn_score</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>219.187219</td>\n",
       "      <td>31</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>399.702142</td>\n",
       "      <td>81</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLAG_OWN_CAR</td>\n",
       "      <td>131.112188</td>\n",
       "      <td>36</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLAG_OWN_REALTY</td>\n",
       "      <td>409.175080</td>\n",
       "      <td>57</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNT_CHILDREN</td>\n",
       "      <td>612.259304</td>\n",
       "      <td>67</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_DAY</td>\n",
       "      <td>280.876199</td>\n",
       "      <td>29</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_WEEK</td>\n",
       "      <td>438.407442</td>\n",
       "      <td>43</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_MON</td>\n",
       "      <td>1334.932854</td>\n",
       "      <td>143</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_QRT</td>\n",
       "      <td>964.461127</td>\n",
       "      <td>106</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_YEAR</td>\n",
       "      <td>3820.227941</td>\n",
       "      <td>404</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature  importance_gain  importance_split  trn_score  \\\n",
       "0            NAME_CONTRACT_TYPE       219.187219                31   0.701949   \n",
       "1                   CODE_GENDER       399.702142                81   0.701949   \n",
       "2                  FLAG_OWN_CAR       131.112188                36   0.701949   \n",
       "3               FLAG_OWN_REALTY       409.175080                57   0.701949   \n",
       "4                  CNT_CHILDREN       612.259304                67   0.701949   \n",
       "..                          ...              ...               ...        ...   \n",
       "115   AMT_REQ_CREDIT_BUREAU_DAY       280.876199                29   0.720379   \n",
       "116  AMT_REQ_CREDIT_BUREAU_WEEK       438.407442                43   0.720379   \n",
       "117   AMT_REQ_CREDIT_BUREAU_MON      1334.932854               143   0.720379   \n",
       "118   AMT_REQ_CREDIT_BUREAU_QRT       964.461127               106   0.720379   \n",
       "119  AMT_REQ_CREDIT_BUREAU_YEAR      3820.227941               404   0.720379   \n",
       "\n",
       "     run  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  \n",
       "..   ...  \n",
       "115    3  \n",
       "116    3  \n",
       "117    3  \n",
       "118    3  \n",
       "119    3  \n",
       "\n",
       "[360 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efb49112d2adb44a3019aa142f8efccc83743425"
   },
   "source": [
    "### Display distribution examples\n",
    "\n",
    "A few plots are better than any words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0905c7e06c936a8cdbc0012bcd1a3a13d1cd5803",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    # Plot Split importances\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n",
    "    # Plot Gain importances\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "1ed28ef87f45a1b9fbf9681250ffd2948c45cb46"
   },
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "556dd360ce384a54eb1babb92f085403d2a45433"
   },
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "2024cc04ab4e53db4cc50db4efe527030b54ee5e"
   },
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "4d7ebb2e105f7ac759a42c54228d1cd7c97fe484"
   },
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "7cacee961a02ee3a9cd1b5ee56ec9b2239714129"
   },
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "30a61af86daa613a6b86a980639b212a6065be2b"
   },
   "source": [
    "From the above plot I believe the power of the exposed feature selection method is demonstrated. In particular it is well known that :\n",
    " - Any feature sufficient variance can be used and made sense of by tree models. You can always find splits that help scoring better\n",
    " - Correlated features have decaying importances once one of them is used by the model. The chosen feature will have strong importance and its correlated suite will have decaying importances\n",
    " \n",
    " The current method allows to :\n",
    "  - Drop high variance features if they are not really related to the target\n",
    "  - Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a99b7b4914185cc4307f17b7d325bdde359101bf"
   },
   "source": [
    "### Score features\n",
    "\n",
    "There are several ways to score features : \n",
    " - Compute the number of samples in the actual importances that are away from the null importances recorded distribution.\n",
    " - Compute ratios like Actual / Null Max, Actual  / Null Mean,  Actual Mean / Null Max\n",
    " \n",
    "In a first step I will use the log actual feature importance divided by the 75 percentile of null distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c95234084a71f516361b685ec2a5872b54322639"
   },
   "outputs": [],
   "source": [
    "feature_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "    feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25512458d6d6b173d04b19e2f0c7e70bdc4fd71c"
   },
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "c9ddcbced0a061e0cdd867ae09f53b822d22a624",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_imp_df.to_csv('null_importances_distribution_rf.csv')\n",
    "actual_imp_df.to_csv('actual_importances_ditribution_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b434ed7f33710271315545c39bae87d9d0cbeee6"
   },
   "source": [
    "### Check the impact of removing uncorrelated features\n",
    "\n",
    "Here I'll use a different metric to asses correlation to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d8a5d1b03e26f61dee356ac5661ade25e1ef155a"
   },
   "outputs": [],
   "source": [
    "correlation_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n",
    "    correlation_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\n",
    "fig.subplots_adjust(top=0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71cb5349912200b11e2c9a386c7cd2a308558781"
   },
   "source": [
    "### Score feature removal for different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "35f30fb61b917e93701ff63b545836d27626a748"
   },
   "outputs": [],
   "source": [
    "def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None):\n",
    "    # Fit LightGBM \n",
    "    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': .1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': 13,\n",
    "        'n_jobs': 4,\n",
    "        'min_split_gain': .00001,\n",
    "        'reg_alpha': .00001,\n",
    "        'reg_lambda': .00001,\n",
    "        'metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    hist = lgb.cv(\n",
    "        params=lgb_params, \n",
    "        train_set=dtrain, \n",
    "        num_boost_round=2000,\n",
    "        categorical_feature=cat_feats,\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        shuffle=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=0,\n",
    "        seed=17\n",
    "    )\n",
    "    # Return the last mean / std values \n",
    "    return hist['auc-mean'][-1], hist['auc-stdv'][-1]\n",
    "\n",
    "# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]\n",
    "# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])\n",
    "\n",
    "for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]:\n",
    "    split_feats = [_f for _f, _score, _ in correlation_scores if _score >= threshold]\n",
    "    split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "    gain_feats = [_f for _f, _, _score in correlation_scores if _score >= threshold]\n",
    "    gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "                                                                                             \n",
    "    print('Results for threshold %3d' % threshold)\n",
    "    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET'])\n",
    "    print('\\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1]))\n",
    "    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET'])\n",
    "    print('\\t GAIN  : %.6f +/- %.6f' % (gain_results[0], gain_results[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "244b4799bb7f88477370beb378b197a1278595da"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
